{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "assert 'GPU' in str(device_lib.list_local_devices())\n",
    "from tensorflow import GPUOptions, Session, ConfigProto\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# confirm Keras sees the GPU\n",
    "from keras import backend\n",
    "assert len(backend.tensorflow_backend._get_available_gpus()) > 0\n",
    "\n",
    "gpu_options = GPUOptions(per_process_gpu_memory_fraction = 0.7)       #Assign portion of memory for training this model\n",
    "sess = Session(config = ConfigProto(gpu_options = gpu_options))\n",
    "\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import datetime\n",
    "#Uncomment to run on CPU\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Galaxy-zoo-2020-04-09-14-01-57.176652\n"
     ]
    }
   ],
   "source": [
    "t = str(datetime.datetime.now())\n",
    "t=t.replace(' ','-')\n",
    "t=t.replace(':','-')\n",
    "NAME = \"Galaxy-zoo-\"+t\n",
    "print(NAME)\n",
    "tensorboard = TensorBoard(log_dir = 'logs\\{}'.format(NAME))\n",
    "DIR = \"F:/Projects/Neural Net/Galaxy Zoo/Data/images_training_rev1\"\n",
    "\n",
    "train_path = DIR + \"\\TrainingFolder\"\n",
    "validate_path = DIR + \"\\ValidationFolder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    'Class1.1', 'Class1.2', 'Class1.3', 'Class2.1', 'Class2.2', 'Class3.1',\n",
    "    'Class3.2', 'Class4.1', 'Class4.2', 'Class5.1', 'Class5.2', 'Class5.3',\n",
    "    'Class5.4', 'Class6.1', 'Class6.2', 'Class7.1', 'Class7.2', 'Class7.3',\n",
    "    'Class8.1', 'Class8.2', 'Class8.3', 'Class8.4', 'Class8.5', 'Class8.6',\n",
    "    'Class8.7', 'Class9.1', 'Class9.2', 'Class9.3', 'Class10.1', 'Class10.2',\n",
    "    'Class10.3', 'Class11.1', 'Class11.2', 'Class11.3', 'Class11.4',\n",
    "    'Class11.5', 'Class11.6'\n",
    "]\n",
    "\n",
    "\n",
    "def append_ext(fn):\n",
    "    return fn + \".jpg\"\n",
    "\n",
    "\n",
    "traindf = pd.read_csv(\"F:/Projects/Neural Net/Galaxy Zoo/Data/training_solutions_rev1/training_solutions_rev1.csv\")\n",
    "traindf[\"id\"] = traindf['GalaxyID'].astype(str).apply(append_ext)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    fill_mode='nearest',\n",
    "    cval=0,\n",
    "    rescale=1. / 255,\n",
    "    rotation_range=120,\n",
    "    width_shift_range=0.25,\n",
    "    height_shift_range=0.25,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    validation_split = 0.2,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1736 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=traindf,\n",
    "    directory=train_path,\n",
    "    x_col=\"id\",\n",
    "    y_col=classes,\n",
    "    subset=\"training\",\n",
    "    batch_size=8,\n",
    "    seed=123,\n",
    "    shuffle=True,\n",
    "    class_mode=\"other\",\n",
    "    target_size=(224, 224))\n",
    "\n",
    "\n",
    "STEP_SIZE_TRAIN = train_generator.n // train_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 276 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "valid_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=traindf,\n",
    "    directory=validate_path,\n",
    "    x_col=\"id\",\n",
    "    y_col=classes,\n",
    "    subset=\"validation\",\n",
    "    batch_size=8,\n",
    "    seed=123,\n",
    "    shuffle=True,\n",
    "    class_mode=\"raw\",\n",
    "    target_size=(224, 224))\n",
    "\n",
    "STEP_SIZE_VALID = valid_generator.n // valid_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model,load_model\n",
    "from keras.applications.resnet import ResNet152\n",
    "from keras.layers import Add,Input,Dense,Dropout,BatchNormalization,Activation,Flatten,Conv2D,MaxPooling2D,ZeroPadding2D,Lambda,AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.optimizers import SGD, RMSprop, Adam, Adagrad\n",
    "from keras.initializers import glorot_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResNet Implementation block\n",
    "\n",
    "def convolution_block(X, f_shape, filters, stage, block, stride = 2):\n",
    "    '''\n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f_shape -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    s -- Integer, specifying the stride to be used\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
    "    '''\n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    " \n",
    "    F1,F2,F3 = filters\n",
    "    \n",
    "    X_shortcut = X\n",
    "    \n",
    "    #Main Path\n",
    "    #first component\n",
    "    X = Conv2D(filters = F1, kernel_size=(1,1), strides = (stride,stride), name = conv_name_base+'2a', \n",
    "               kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base+'2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    #Second Component\n",
    "    X = Conv2D(filters = F2, kernel_size = (f_shape,f_shape), strides = (1,1), padding = 'same', name = conv_name_base+'2b', \n",
    "               kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base+'2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    #Third Component\n",
    "    X = Conv2D(filters = F3, kernel_size = (1,1), strides = (1,1), padding = 'valid', name = conv_name_base+'2c', \n",
    "               kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base+'2c')(X)\n",
    "    \n",
    "    #Shortcut Path\n",
    "    X_shortcut = Conv2D(filters = F3, kernel_size = (1,1), strides = (stride,stride), padding = 'valid', name = conv_name_base+'1',\n",
    "                       kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis = 3, name = bn_name_base+'1')(X_shortcut)\n",
    "    \n",
    "    #Final\n",
    "    X = Add()([X,X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "def identity_block(X, f_shape, filters, stage, block):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    F1,F2,F3 = filters\n",
    "    \n",
    "    X_shortcut = X\n",
    "    \n",
    "    #Main Path\n",
    "    #first component\n",
    "    X = Conv2D(filters = F1, kernel_size = (1,1), strides = (1,1), padding = 'valid', name = conv_name_base+\"2a\",\n",
    "              kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base+'2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    #second component\n",
    "    X = Conv2D(filters = F2, kernel_size = (f_shape,f_shape), strides = (1,1), padding = 'same', name = conv_name_base+\"2b\",\n",
    "              kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base+'2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    #Third component\n",
    "    X = Conv2D(filters = F3, kernel_size = (1,1), strides = (1,1), padding = 'valid', name = conv_name_base+'2c',\n",
    "              kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base+'2c')(X)\n",
    "    \n",
    "    #Final\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "def ResNet50(input_shape = (106,106,3), classes = 37):\n",
    "    \"\"\"\n",
    "    Implementation of the popular ResNet50 the following architecture:\n",
    "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
    "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    #Create a tensor of shape (106,106,3)\n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    #Zero padding to the tensor\n",
    "    X = ZeroPadding2D((3,3))(X_input)\n",
    "    \n",
    "    #Stage 1\n",
    "    X = Conv2D(64, (7,7), strides = (2,2), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3,3), strides = (2,2))(X)\n",
    "    \n",
    "    #Stage 2\n",
    "    X = convolution_block(X,f_shape = 3, filters = [64,64,256], stage = 2, block = 'a', stride = 1)\n",
    "    X = identity_block(X, f_shape = 3, filters = [64,64,256], stage = 2, block = 'b')\n",
    "    X = identity_block(X,3,[64,64,256], stage = 2, block = 'c')\n",
    "    \n",
    "    #Stage 3\n",
    "    X = convolution_block(X, f_shape = 3, filters = [128,128,512], stage = 3, block = 'a', stride = 2)\n",
    "    X = identity_block(X, 3, [128,128,512], stage = 3, block = 'b')\n",
    "    X = identity_block(X, 3, [128,128,512], stage = 3, block = 'c')\n",
    "    X = identity_block(X, 3, [128,128,512], stage = 3, block = 'd')\n",
    "    \n",
    "    #Stage 4\n",
    "    X = convolution_block(X, f_shape = 3, filters = [256,256,1024], stage = 4, block = 'a', stride = 2)\n",
    "    X = identity_block(X, 3, [256,256,1024], stage = 4, block = 'b')\n",
    "    X = identity_block(X, 3, [256,256,1024], stage = 4, block = 'c')\n",
    "    X = identity_block(X, 3, [256,256,1024], stage = 4, block = 'd')\n",
    "    X = identity_block(X, 3, [256,256,1024], stage = 4, block = 'e')\n",
    "    X = identity_block(X, 3, [256,256,1024], stage = 4, block = 'f')\n",
    "    \n",
    "    #Stage 5\n",
    "    X = convolution_block(X, f_shape = 3, filters = [512,512,2048], stage = 5, block = 'a', stride = 2)\n",
    "    X = identity_block(X, 3, [512,512,2048], stage = 5, block = 'b')\n",
    "    X = identity_block(X, 3, [512,512,2048], stage = 5, block = 'c')\n",
    "    \n",
    "    '''#Stage 6\n",
    "    X = convolution_block(X, f_shape = 3, filters = [1024,1024,4096], stage = 6, block = 'a', stride = 2)\n",
    "    X = identity_block(X, 3, [1024,1024,4096], stage = 6, block = 'b')\n",
    "    X = identity_block(X, 3, [1024,1024,4096], stage = 6, block = 'c')\n",
    "    X = identity_block(X, 3, [1024,1024,4096], stage = 6, block = 'd')\n",
    "    X = identity_block(X, 3, [1024,1024,4096], stage = 6, block = 'e')\n",
    "    X = identity_block(X, 3, [1024,1024,4096], stage = 6, block = 'f')'''\n",
    "    \n",
    "    #Average Pooling\n",
    "    X = AveragePooling2D((2,2), name = \"avg_pool\")(X)\n",
    "    \n",
    "    \n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation = 'sigmoid', name = 'fc'+str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    #create a model instance\n",
    "    model = Model(inputs = X_input, outputs = X, name = 'ResNet50')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\shrey\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\shrey\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\shrey\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "A local file was found, but it seems to be incomplete or outdated because the auto file hash does not match the original value of ee4c566cf9a93f14d82f913c2dc6dd0c so we will re-download the data.\n",
      "Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet152_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "234700800/234698864 [==============================] - 1448s 6us/step\n"
     ]
    }
   ],
   "source": [
    "img_shape = (224, 224, 3)\n",
    "\n",
    "resnet_model = ResNet152(include_top=False, input_shape=img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model = ResNet50(input_shape = (224,224,3), classes = 37)\\n\\nmodel.compile(optimizer=Adam(lr=0.001, decay=5e-4), loss=\\'mse\\', metrics=[\"accuracy\"])'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten output of last layer before adding output layer (Dense layer)\n",
    "x = Flatten()(resnet_model.output)\n",
    "\n",
    "# Add output layer (number of outputs = 37)\n",
    "x = Dense(len(classes), activation='sigmoid')(x)\n",
    "\n",
    "# Load the modified model\n",
    "model = Model(inputs=resnet_model.input, outputs=x)\n",
    "\n",
    "'''model = ResNet50(input_shape = (224,224,3), classes = 37)\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.001, decay=5e-4), loss='mse', metrics=[\"accuracy\"])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\shrey\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = Adam(lr=0.001, decay=5e-4)\n",
    "\n",
    "model.compile(optimizer, loss='mse', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', patience=4, verbose=1, mode='auto')\n",
    "\n",
    "history = LossHistory()\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath='F:/Projects/Neural Net/Galaxy Zoo/Data/weights.hdf5', verbose=2, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "217/217 [==============================] - 115s 531ms/step - loss: 0.0410 - acc: 0.4902 - val_loss: 0.0598 - val_acc: 0.2353\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05981, saving model to F:/Projects/Neural Net/Galaxy Zoo/Data/weights.hdf5\n",
      "Epoch 2/20\n",
      "217/217 [==============================] - 111s 511ms/step - loss: 0.0332 - acc: 0.5438 - val_loss: 0.0758 - val_acc: 0.0075\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.05981\n",
      "Epoch 3/20\n",
      "217/217 [==============================] - 115s 531ms/step - loss: 0.0345 - acc: 0.5478 - val_loss: 0.0429 - val_acc: 0.4403\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.05981 to 0.04293, saving model to F:/Projects/Neural Net/Galaxy Zoo/Data/weights.hdf5\n",
      "Epoch 4/20\n",
      "217/217 [==============================] - 114s 523ms/step - loss: 0.0316 - acc: 0.5668 - val_loss: 0.0372 - val_acc: 0.4888\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04293 to 0.03719, saving model to F:/Projects/Neural Net/Galaxy Zoo/Data/weights.hdf5\n",
      "Epoch 5/20\n",
      "217/217 [==============================] - 114s 524ms/step - loss: 0.0304 - acc: 0.5432 - val_loss: 0.0416 - val_acc: 0.4552\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.03719\n",
      "Epoch 6/20\n",
      "217/217 [==============================] - 114s 528ms/step - loss: 0.0312 - acc: 0.5628 - val_loss: 0.0315 - val_acc: 0.5896\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.03719 to 0.03153, saving model to F:/Projects/Neural Net/Galaxy Zoo/Data/weights.hdf5\n",
      "Epoch 7/20\n",
      "217/217 [==============================] - 113s 522ms/step - loss: 0.0306 - acc: 0.5726 - val_loss: 0.0310 - val_acc: 0.5597\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.03153 to 0.03100, saving model to F:/Projects/Neural Net/Galaxy Zoo/Data/weights.hdf5\n",
      "Epoch 8/20\n",
      "217/217 [==============================] - 113s 522ms/step - loss: 0.0300 - acc: 0.5732 - val_loss: 0.0311 - val_acc: 0.5634\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.03100\n",
      "Epoch 9/20\n",
      "217/217 [==============================] - 114s 523ms/step - loss: 0.0324 - acc: 0.5530 - val_loss: 0.0265 - val_acc: 0.5448\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.03100 to 0.02649, saving model to F:/Projects/Neural Net/Galaxy Zoo/Data/weights.hdf5\n",
      "Epoch 10/20\n",
      "217/217 [==============================] - 113s 519ms/step - loss: 0.0313 - acc: 0.5444 - val_loss: 0.0235 - val_acc: 0.5821\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02649 to 0.02352, saving model to F:/Projects/Neural Net/Galaxy Zoo/Data/weights.hdf5\n",
      "Epoch 11/20\n",
      "217/217 [==============================] - 113s 520ms/step - loss: 0.0312 - acc: 0.5518 - val_loss: 0.0467 - val_acc: 0.4104\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.02352\n",
      "Epoch 12/20\n",
      "217/217 [==============================] - 131s 604ms/step - loss: 0.0302 - acc: 0.5599 - val_loss: 0.0254 - val_acc: 0.5373\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.02352\n",
      "Epoch 13/20\n",
      "217/217 [==============================] - 114s 525ms/step - loss: 0.0307 - acc: 0.5616 - val_loss: 0.0305 - val_acc: 0.5597\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.02352\n",
      "Epoch 14/20\n",
      "217/217 [==============================] - 113s 522ms/step - loss: 0.0297 - acc: 0.5639 - val_loss: 0.0296 - val_acc: 0.4291\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.02352\n",
      "Epoch 00014: early stopping\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=STEP_SIZE_VALID,\n",
    "    epochs=20,\n",
    "    callbacks=[history, checkpointer, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(hist.epoch, hist.history['loss'], label='Training Loss')\n",
    "plt.plot(\n",
    "    hist.epoch, hist.history['val_loss'], label='Validation', linestyle='--')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('weights.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
